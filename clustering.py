# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pBImkq5kgmqi-0ixSc4Hr47nYOLobLMQ

# importing libs
"""

from keras.preprocessing.image import load_img 
from keras.preprocessing.image import img_to_array 
from keras.applications.vgg16 import preprocess_input 

from keras.applications.vgg16 import VGG16 
from keras.models import Model

from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.cluster import MiniBatchKMeans
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn import metrics
from cv2 import resize, INTER_LINEAR

import os
import numpy as np
import matplotlib.pyplot as plt
from random import randint
import pandas as pd
import pickle
from scipy.special import comb
from itertools import combinations

"""# Evaluation Tools"""

def rand_index_score(actual_labels, predicted_labels):
    tp_plus_fp = comb(np.bincount(actual_labels), 2).sum() # true positive + false positive
    tp_plus_fn = comb(np.bincount(predicted_labels), 2).sum() # true positive + false negative

    A = np.c_[(actual_labels, predicted_labels)] # Translates slice objects to concatenation along the second axis

    tp = sum(comb(np.bincount(A[A[:, 0] == i, 1]), 2).sum() for i in set(actual_labels))
    fp = tp_plus_fp - tp
    fn = tp_plus_fn - tp
    tn = comb(len(A), 2) - tp - fp - fn
    return (tp + tn) / (tp + fp + fn + tn)

"""compute rand index, from cluster arg which are actual label, and classes which are predicted labels by the algorithm.

![0ec80af7aba3d18700e0c06940c3f277990f95c7.svg](data:image/svg+xml;base64,<svg xmlns:xlink="http://www.w3.org/1999/xlink" width="29.764ex" height="5.343ex" style="vertical-align: -2.005ex;" viewBox="0 -1437.2 12814.9 2300.3" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">{\displaystyle RI={\frac {TP+TN}{TP+FP+FN+TN}}}</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-52" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path>
<path stroke-width="1" id="E1-MJMATHI-49" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path>
<path stroke-width="1" id="E1-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path>
<path stroke-width="1" id="E1-MJMATHI-54" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path>
<path stroke-width="1" id="E1-MJMATHI-50" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path>
<path stroke-width="1" id="E1-MJMATHI-4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path>
<path stroke-width="1" id="E1-MJMATHI-46" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-52" x="0" y="0"></use>
 <use xlink:href="#E1-MJMATHI-49" x="759" y="0"></use>
 <use xlink:href="#E1-MJMAIN-3D" x="1541" y="0"></use>
<g transform="translate(2598,0)">
<g transform="translate(120,0)">
<rect stroke="none" width="9976" height="60" x="0" y="220"></rect>
<g transform="translate(2852,676)">
 <use xlink:href="#E1-MJMATHI-54" x="0" y="0"></use>
 <use xlink:href="#E1-MJMATHI-50" x="704" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2B" x="1678" y="0"></use>
 <use xlink:href="#E1-MJMATHI-54" x="2678" y="0"></use>
 <use xlink:href="#E1-MJMATHI-4E" x="3383" y="0"></use>
</g>
<g transform="translate(60,-704)">
 <use xlink:href="#E1-MJMATHI-54" x="0" y="0"></use>
 <use xlink:href="#E1-MJMATHI-50" x="704" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2B" x="1678" y="0"></use>
 <use xlink:href="#E1-MJMATHI-46" x="2678" y="0"></use>
 <use xlink:href="#E1-MJMATHI-50" x="3428" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2B" x="4402" y="0"></use>
 <use xlink:href="#E1-MJMATHI-46" x="5402" y="0"></use>
 <use xlink:href="#E1-MJMATHI-4E" x="6152" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2B" x="7263" y="0"></use>
 <use xlink:href="#E1-MJMATHI-54" x="8263" y="0"></use>
 <use xlink:href="#E1-MJMATHI-4E" x="8968" y="0"></use>
</g>
</g>
</g>
</g>
</svg>)

*  bincount -> Count number of occurrences of each value in array of non-negative ints.


*  comb -> combination
"""

class Cluster:

  def __init__(self, cluster_id: int):
    self.predicted_file_names = []
    self.actual_file_names_labels = []
    self.cluster_id = cluster_id # integer
    self.predicted_label = None
  
  def infer_cluster_label(self):
    self.predicted_label = max(self.actual_file_names_labels, key=self.actual_file_names_labels.count)

  
  def predicted_and_actual_label_list(self):
    if not self.predicted_label:
      raise Exception('predicted label was not inferred')
    preds = [self.predicted_label] * len(self.predicted_file_names)

    return self.actual_file_names_labels, preds
  
  @staticmethod
  def build_cluster_objects_from(algorithm, persons_file_names, actual_unique_labels):
    clusters = [] # indexes are cluster id
    
    # each algorithm has different attribute so we should check for cluster number attr
    if hasattr(algorithm, 'n_clusters') and algorithm.n_clusters is not None:
      n_clusters = algorithm.n_clusters
    elif hasattr(algorithm, 'n_clusters_') and algorithm.n_clusters_ is not None:
      n_clusters = algorithm.n_clusters_
    else:
      n_clusters = len(set(algorithm.labels_))
    
    # create a Cluster object for number of cluster founded
    for cluster_id in range(1, n_clusters + 1):
      clusters.append(Cluster(cluster_id=cluster_id))
    
    # each cluster label is for the data we provided in the same index so we zip original data, labels and their actual labels
    # update each cluster with its own members
    for file_name, predicted_cluster_id, actual_label in zip(persons_file_names, algorithm.labels_, actual_unique_labels):
      clusters[predicted_cluster_id - 1].predicted_file_names.append(file_name) # append file while it is predicted for this cluster 
      clusters[predicted_cluster_id - 1].actual_file_names_labels.append(actual_label) # append the real label for this file name
    
    return clusters

"""This class holds each cluster data including labels, datas, actual labels and inffered label for cluster. """

class ClusterWorker:

  def __init__(self, algorithm, features):
    self.algorithm = algorithm
    self.features = features
    self.flattened_predicted_labels = np.array([]).astype(np.int64)
    self.flattened_actual_label = np.array([]).astype(np.int64)

  def perform(self):
    self.algorithm.fit(self.features)

    # if hasattr(self.algorithm, 'n_clusters') and self.algorithm.n_clusters is not None:
    #   n_clusters = self.algorithm.n_clusters
    # elif hasattr(self.algorithm, 'n_clusters_') and self.algorithm.n_clusters_ is not None:
    #   n_clusters = self.algorithm.n_clusters_
    # else:
    #   n_clusters = len(set(self.algorithm.labels_))
    # print(n_clusters)

    self._infer_lables()
  
  def _infer_lables(self):
    # generate Clusters
    kmeans_clusters = Cluster.build_cluster_objects_from(self.algorithm, persons_file_names, unique_labels_list)

    preds = np.array([]).astype(np.int64)
    acs = np.array([]).astype(np.int64)
    
    # unpack each cluster data with their infered label
    for cl in kmeans_clusters:
      cl.infer_cluster_label()
      ac, pred = cl.predicted_and_actual_label_list()
      preds = np.append(preds, pred)
      acs = np.append(acs, ac)
    
    # flat labels so that we process them
    self.flattened_predicted_labels = preds.flatten()
    self.flattened_actual_label = acs.flatten()

  def n_cluster(self):
    return len(set(self.algorithm.labels_))
  
  def rand_index(self):
    return metrics.adjusted_rand_score(self.flattened_actual_label, self.flattened_predicted_labels)

  def self_implemented_rand_index(self):
    return rand_index_score(self.flattened_actual_label, self.flattened_predicted_labels)

  def purity(self):
    return metrics.accuracy_score(self.flattened_actual_label, self.flattened_predicted_labels)

  def print_statictics(self):
    print('Predicted labels: {}'.format(self.flattened_predicted_labels[:30]))
    print('Actual labels:    {}'.format(self.flattened_actual_label[:30]))
    print('Purity: {}'.format(metrics.accuracy_score(self.flattened_actual_label, self.flattened_predicted_labels)))
    print('Rand Index: {}'.format(metrics.adjusted_rand_score(self.flattened_actual_label, self.flattened_predicted_labels)))
    print('Rand Index self implemenetd: {}'.format(rand_index_score(self.flattened_actual_label, self.flattened_predicted_labels)))
    print()

"""This is a wrapper for each algorithm we use.
it can infer clusters labels, purity and rand index.
"""

class ClusterStatictics:

  def measure(self):
    pass

  def plot_statistics(self):
    pass

"""Abstract class for measuring statistics

# Loading Data
"""

class DataLoader:

  dataSetPath = '/content/drive/MyDrive/AI/Clustering/ORL'

  @staticmethod
  def generate_raw_dataset(persons_file_names, model):
    data = {}
    for person_name in persons_file_names:
      try:
        feat = DataLoader.extract_features(person_name, model)
        data[person_name] = feat
      except:
        continue
    return data

  @staticmethod
  def extract_features(file, model):
    # feature extractor method can use cnn or pixels
    file_path = DataLoader.dataSetPath + '/' + file + '.jpg'

    img = load_img(file_path, target_size=(224,224))

    img = np.array(img)

    if model:
      reshaped_img = img.reshape(1,224,224,3)

      imgx = preprocess_input(reshaped_img)

      features = model.predict(imgx, use_multiprocessing=True)

      return features
    else:
      # in order to use pixel we should reshape it to 2-dim array from 3-dim
      img = img.reshape(img.shape[0], (img.shape[1]*img.shape[2]))

      return img

  @staticmethod
  def generate_vgg16_network():
    # cnn model which we can use to improve clustering
    cnn = VGG16()
    model = Model(inputs = cnn.inputs, outputs = cnn.layers[-2].output)
    return model

  @staticmethod
  def generate_file_names_without_extension(path= r"/content/drive/MyDrive/AI/Clustering/ORL"):
    # Load filenames without .jpg extension
    os.chdir(path)
    persons_file_names = []
    
    with os.scandir(path) as files:
      for file in files:
        if file.name.endswith('.jpg'):
          base = os.path.basename(file)
          name = os.path.splitext(base)[0]
          persons_file_names.append(name)
    return persons_file_names

persons_file_names = DataLoader.generate_file_names_without_extension()

"""Load filenames without .jpg extension"""

data = DataLoader.generate_raw_dataset(persons_file_names, None)

"""extract raw features for each image and store them in dic

# Data Preprocessing
"""

def find_images_labels(persons_file_names):
  ids = list()
  for name in persons_file_names:
    splited = name.split('_')[1]
    ids.append(int(splited))
  
  return ids, set(ids)

"""extract label from image name"""

filenames = np.array(list(data.keys()))

feat = np.array(list(data.values()))

# create 2 dim features
feat = feat.reshape(feat.shape[0], (feat.shape[1]*feat.shape[2]))

# feat = feat.reshape(-1,4096)

unique_labels_list, unique_labels_set = find_images_labels(persons_file_names)

"""preapare data for being used in algorithm"""

pca = PCA(n_components=100, random_state=22)
pca.fit(feat)
transformed_pca = pca.transform(feat)

"""dimensionality reduction for faster computation

# KMeans
"""

k_means_mini_batch = ClusterWorker(MiniBatchKMeans(n_clusters=len(unique_labels_set), random_state=22), feat)
k_means_mini_batch.perform()
k_means_mini_batch.print_statictics()

k_means = ClusterWorker(KMeans(n_clusters=len(unique_labels_set), random_state=22), feat)
k_means.perform()
k_means.print_statictics()

"""The above code is for testing the clustering. 
I once cluster the dataset using the K-Means-MiniBatch algorithm and once use the normal K-Means. 
The difference between these two algorithm is that the MiniBatch algorithm is much faster than the normal K-means.

But the MiniBatch algorithm has a lower Purity and Rand Index, which is visible when you run the above code. 
"""

class KmeansClusterStatictics(ClusterStatictics):

  def __init__(self, features):
    self.features = features
    self.clusters = [10, 20, 40, 60, 80, 100]
    self.rand_indexes = []
    self.purities = []
  
  def measure(self):
    for i in self.clusters:
      k_means_mini_batch = ClusterWorker(MiniBatchKMeans(n_clusters=i, random_state=22), self.features)
      k_means_mini_batch.perform()
      
      purity = k_means_mini_batch.purity()
      rand_index = k_means_mini_batch.rand_index()
      
      self.rand_indexes.append(rand_index)
      self.purities.append(purity)

  def plot_statistics(self):
    plt.plot(self.clusters, self.purities)
    plt.suptitle('Effect Of Cluster Count On Purity')
    plt.show()
    plt.plot(self.clusters, self.rand_indexes)
    plt.suptitle('Effect Of Cluster Count On Rand-Index')
    plt.show()

kmeans_stat = KmeansClusterStatictics(features=feat)
kmeans_stat.measure()
kmeans_stat.plot_statistics()

"""The above code is MiniBatch K-Means but with differnet values for n_cluster. 
We can see that with more clusters, we gain more purity and rand-index.

# DBScan
"""

class EpsilonPredictor:

  def __init__(self, features):
    self.features = features
    self.neighbour = NearestNeighbors(n_neighbors=2)
    self.distances = []
  
  def measure(self):
    nbrs = self.neighbour.fit(self.features)
    self.distances, indices = nbrs.kneighbors(self.features)
    self.distances = np.sort(self.distances, axis=0)
    self.distances = self.distances[:,1]
  
  def plot_result(self):
    plt.suptitle('Datas in Distance Range')
    plt.xlabel("Data")
    plt.ylabel("Epsilons")
    plt.plot(self.distances)
    plt.show()

eps_predictor = EpsilonPredictor(feat)
eps_predictor.measure()
eps_predictor.plot_result()

"""The above code is for figuring out that which epsilon is ideal for our dataset.<br>

To estimate an ideal epsilon, I used the NearestNeighbor library. first I find the differences between all the data, then by sorting it, we can find that how much distance the datas have. 
For example as you can see in the plot, lower than distance 7000 and higher than distance 15000 will decrease the accuracy of our cluster, because most of our data are between the range 7000 to 15000. 
For example as you can see in the plot, lower than distance 7000 and higher than distance 6 will decrease the accuracy of our cluster, because most of our data are between the range 7000 to 15000. 
In other words, less than distance 7000, algorithm will count most of our data as noisy, and more than distance , algorithm will put most of our data in one cluster. 
In the next code we will try different epsilons between 5000 to 15000.
"""

class DBSCANQuality:

  def __init__(self, features):
    self.features = features
    self.epsilons = [ep for ep in range(5000,16000, 500)]
    self.rand_indexes = []
    self.purities = []
    self.n_clusters = []
  
  def measure(self):
    for ep in self.epsilons:
      dbscan = ClusterWorker(DBSCAN(min_samples=1, eps=ep), features=self.features)
      dbscan.perform()
      # dbscan.print_statictics()

      purity = dbscan.purity()
      rand_index = dbscan.rand_index()
      n_cluster = dbscan.n_cluster()

      self.rand_indexes.append(rand_index)
      self.purities.append(purity)
      self.n_clusters.append(n_cluster)
  
  def plot_result(self):
    plt.plot(self.epsilons, self.purities)
    plt.suptitle('Epsilon on Purity')
    plt.xlabel("Epsilons")
    plt.ylabel("Purities")
    plt.show()

    plt.plot(self.epsilons, self.rand_indexes)
    plt.suptitle('Epsilon on Rand-Index')
    plt.xlabel("Epsilons")
    plt.ylabel("Rand Indexes")
    plt.show()

    plt.plot(self.epsilons, self.n_clusters)
    plt.suptitle('Epsilon on Number of Clusters')
    plt.xlabel("Epsilons")
    plt.ylabel("Clsuters")
    plt.show()

eps_predictor = DBSCANQuality(feat)
eps_predictor.measure()
eps_predictor.plot_result()

"""We can see that for epsilons lower than 7000, we had a cluster for each data, which results into high purity and rand-index but not usefull. and for epsilons more than 15000 we get less clusters which results into bad accuracy and purity. <br>
But if we choose epsilons between 7000 to 15000, it's more ideal. and according to the plots, epsilon 12000 is the best for this dataset.
"""

class DBSCANNoise:

  def __init__(self, features):
    self.features = features
    self.min_samples = [1, 2, 3, 4, 5]
    self.noise_percentage = []
  
  def measure(self):
    for i in self.min_samples:
      dbscan = DBSCAN(eps=12000, min_samples=i)
      dbscan.fit(self.features)
      index = np.where(dbscan.labels_ == -1) # noisy datas are labeled as -1
      self.noise_percentage.append(len(index[0])/1000)
  
  def plot_result(self):
    plt.plot(self.min_samples, self.noise_percentage)
    plt.suptitle('Minimum samples on Noisy data percentage')
    plt.xlabel("Min Samples")
    plt.ylabel("Noise Percentage")
    plt.show()

db_noise_mes = DBSCANNoise(feat)
db_noise_mes.measure()
db_noise_mes.plot_result()

"""shows the effect of min sample on the amount of noisy datas recognized.

# Agglomerative
"""

class AgglomerativeThreshHoldStat(ClusterStatictics):

  def __init__(self, features):
    self.features = features
    self.thresholds = [i for i in range(10000,26000, 1000)]
    self.rand_indexes = []
    self.purities = []
    self.n_clusters = []
  
  def measure(self):
    for i in self.thresholds:
      al = AgglomerativeClustering(distance_threshold=i, n_clusters=None, compute_full_tree=True)
      agglo = ClusterWorker(al, features=self.features)
      agglo.perform()
      # agglo.print_statictics()

      purity = agglo.purity()
      rand_index = agglo.rand_index()
      n_cluster = agglo.n_cluster()
      self.rand_indexes.append(rand_index)
      self.purities.append(purity)
      self.n_clusters.append(n_cluster)

  def plot_statistics(self):
    plt.plot(self.thresholds, self.purities)
    plt.xlabel("Thresholds")
    plt.ylabel("Purities")
    plt.suptitle('Threshold on Purity')
    plt.show()
    plt.plot(self.thresholds, self.rand_indexes)
    plt.suptitle('Threshold on Rand-Index')
    plt.xlabel("Thresholds")
    plt.ylabel("Rand-Indexes")
    plt.show()
    plt.plot(self.thresholds, self.n_clusters)
    plt.suptitle('Threshold on Number of Clusters')
    plt.xlabel("Thresholds")
    plt.ylabel("Clsuters")
    plt.show()

agllo_threshold_stats = AgglomerativeThreshHoldStat(feat)
agllo_threshold_stats.measure()
agllo_threshold_stats.plot_statistics()

"""As you can see, by increasing the threshold, we gain less purity and rand-index. <br>
The ideal amount for threshold is between 16000 to 22000, because the number of clusters are acceptable and also the purity and rand-index is very good. <br>
"""

class AgglomerativeStat(ClusterStatictics):

  def __init__(self, features):
    self.features = features
    self.n_clusters = [10, 20, 41, 50, 60, 70]
    self.rand_indexes = []
    self.purities = []
  
  def measure(self):
    for i in self.n_clusters:
      agglo = ClusterWorker(AgglomerativeClustering(n_clusters=i),
                            features=self.features)
      agglo.perform()
      # agglo.print_statictics()

      purity = agglo.purity()
      rand_index = agglo.rand_index()
      self.rand_indexes.append(rand_index)
      self.purities.append(purity)

  def plot_statistics(self):
    plt.plot(self.n_clusters, self.purities)
    plt.suptitle('Number of Cluster on Purity')
    plt.xlabel("Cluster")
    plt.ylabel("Purities")
    plt.show()

    plt.plot(self.n_clusters, self.rand_indexes)
    plt.suptitle('Number of Cluster on Rand-Index')
    plt.xlabel("Cluster")
    plt.ylabel("Rand-Indexes")
    plt.show()

agllo_stats = AgglomerativeStat(feat)
agllo_stats.measure()
agllo_stats.plot_statistics()

"""Another way of agglomerative algorithm is to define the maximum number of clusters, just like K-means, and the algorithm will do clustering until number of clusters reach the limit. <br>
You can see that after 50 clusters, we won't gain that much purity and rand-index comparing to previous numbers. 
"""

class AgglomerativeTypeStat(ClusterStatictics):

  def __init__(self, features):
    self.features = features
    self.types = ['ward', 'complete', 'average', 'single']
    self.rand_indexes = []
    self.purities = []
  
  def measure(self):
    for t in self.types:
      agglo = ClusterWorker(AgglomerativeClustering(n_clusters=41, linkage=t),
                            features=self.features)
      agglo.perform()
      # agglo.print_statictics()

      purity = agglo.purity()
      rand_index = agglo.rand_index()

      self.rand_indexes.append(rand_index)
      self.purities.append(purity)

  def plot_statistics(self):
    plt.plot(self.types, self.purities)
    plt.suptitle('Type on Purity')
    plt.xlabel("Types")
    plt.ylabel("Purities")
    plt.show()

    plt.plot(self.types, self.rand_indexes)
    plt.suptitle('Type on Rand-Index')
    plt.xlabel("Types")
    plt.ylabel("Rand-Index")
    plt.show()

agllo_type_stats = AgglomerativeTypeStat(feat)
agllo_type_stats.measure()
agllo_type_stats.plot_statistics()

"""The agglomerative algorithm has 4 different types. <br>
The default value for the type is 'ward' and up to now we have used this type for our previous examples. <br>
Here in this example we can see the effect of the type on the accuracy of our clustering. <br>
As you can see the complete and average doesn't have much difference, the ward type is the best and the single type is the worst.
"""